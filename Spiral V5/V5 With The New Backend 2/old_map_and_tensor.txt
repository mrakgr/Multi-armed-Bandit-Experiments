inl CudaTensor =
    inl CudaDeviceVariable = ManagedCuda."CudaDeviceVariable`1"

    inl total_size = Tuple.foldl (inl s x -> s * HostTensor.dim_size x) 1

    inl create {layout ty size} =
        inl size1d = 
            match size with
            | _ :: _ -> total_size size |> SizeT
            | x -> SizeT x
        inl create ty = CudaDeviceVariable ty size1d
        match layout with
        | .aot -> {layout size ar = create ty}
        | .toa -> {layout size ar = HostTensor.toa_map create ty}

    inl from_host_array ar =
        inl t = CudaDeviceVariable (ar.elem_type) (Array.length ar |> SizeT)
        t.CopyToDevice ar
        t

    inl from_host_tensor {size ar layout} = 
        match layout with
        | .aot -> { size layout ar = from_host_array ar}
        | .toa -> { size layout ar = HostTensor.toa_map from_host_array ar }

    inl to_host_array x =
        inl size = x.get_Size()
        inl t = Array.create (x.elem_type) (Operators(.int, size, size))
        x.CopyToHost t
        context.Synchronize()
        t

    inl to_host_tensor {size ar layout} =
        match layout with
        | .aot -> { size layout ar = to_host_array ar }
        | .toa -> { size layout ar = HostTensor.toa_map to_host_array ar }

    inl ptr dev_var = 
        inl x = dev_var.get_DevicePointer()
        inl t = dev_var.elem_type
        !UnsafeCoerceToArrayCudaGlobal(x,t)     

    inl to_device_tensor_form {size ar layout} =
        match layout with
        | .aot -> {size layout ar = ptr ar}
        | .toa -> {size layout ar = HostTensor.toa_map ptr ar}

    inl elem_type {ar layout} =
        match layout with
        | .aot -> ar.elem_type
        | .toa -> HostTensor.toa_map (inl x -> x.elem_type) ar

    inl zip = function
        | x :: xs as l ->
            inl {size=sa layout=la} = x
            Tuple.iter (inl {size=sb layout=lb} -> 
                assert (sa=sb) "The sizes of all the tensors in zip must be the same in order to be zipped"
                assert (eq_type la lb) "The layouts of all the tensors must have the same format."
                )
            match la with
            | .aot -> error_type "Array of tuples tensor layout is currently not supported."
            | .toa -> {size=sa layout=la ar = Tuple.map (inl {ar} -> ar) l}
        | () -> error_type "Empty input to zip is invalid."
        | x -> x
        
    inl coerce_to_1d {size layout ar} = {layout ar size={from=0; to=total_size size - 1} :: ()}

    {create from_host_tensor to_host_tensor zip elem_type coerce_to_1d to_device_tensor_form total_size} |> stack

open CudaTensor
inl map f (!zip in) =
    inl out = create {in with ty = type (f (elem_type in))}

    inl in', out' = coerce_to_1d in |> to_device_tensor_form, coerce_to_1d out |> to_device_tensor_form
    inl near_to = total_size (in'.size)

    run {
        blockDim = 128
        gridDim = 32
        kernel = cuda // Lexical scoping rocks.
            inl from = blockIdx.x * blockDim.x + threadIdx.x
            inl by = gridDim.x * blockDim.x
            Loops.for {from near_to by body=inl {i} ->
                HostTensor.set_unsafe in' i (f (HostTensor.index_unsafe out' i))
                }
        } |> ignore

    out
